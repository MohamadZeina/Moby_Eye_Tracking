{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moby Eye Tracking\n",
    "\n",
    "Notebook for developing fast, accurate eye tracking straight from your webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import face_recognition\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "from tkinter import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras \n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, Dense, MaxPool2D, Flatten \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience functions\n",
    "def small_dot(tkinter_canvas, centre_x, centre_y, radius=5, fill=\"red\"):\n",
    "    \"\"\"Given the centre point of a dot, this convenience function will draw a small dot with given radius\"\"\"\n",
    "    \n",
    "    tkinter_canvas.create_oval(centre_x - radius, centre_y - radius,\n",
    "                               centre_x + radius, centre_y + radius, fill=fill)\n",
    "    \n",
    "    return\n",
    "\n",
    "def random_dot(tkinter_canvas, tk_width, tk_height):\n",
    "    \n",
    "    border = 5 # Should be same, or higher than radius of dots\n",
    "    \n",
    "    random_width = random.randint(border, tk_width - border)\n",
    "    random_height = random.randint(border, tk_height - border)\n",
    "    \n",
    "    small_dot(tkinter_canvas, random_width, random_height)\n",
    "    \n",
    "    return random_width, random_height\n",
    "\n",
    "def neural_model(dummy_sample):\n",
    "    \n",
    "    print(\"About to initialise a neural network with input shape: \", dummy_sample.shape)\n",
    "    \n",
    "    visible = Input(shape=(dummy_sample.shape))\n",
    "    \n",
    "    c11 = Conv2D(8, 3)(visible)\n",
    "    c12 = Conv2D(8, 3)(c11)\n",
    "    p1 = Conv2D(16, 1, strides=2)(c12)\n",
    "    c21 = Conv2D(16, 3)(p1)\n",
    "    c22 = Conv2D(16, 3)(c21)\n",
    "    p2 = Conv2D(32, 1, strides=2)(c22)\n",
    "    #c31 = Conv2D(8, 3)(p2)\n",
    "    #c32 = Conv2D(8, 3)(c31)\n",
    "    #p3 = Conv2D(16, 1, strides=2)(c32)\n",
    "    \n",
    "    f1 = Flatten()(p2)\n",
    "    d1 = Dense(50, activation=\"relu\")(f1)\n",
    "    d2 = Dense(50, activation=\"relu\")(d1)\n",
    "    output = Dense(2)(d2)\n",
    "    \n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    \n",
    "    model.compile(loss=keras.losses.MeanSquaredError(), optimizer=\"adam\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_facial_features(frame, display=False):\n",
    "    \n",
    "    # Basic code for facial landmark extraction from webcam from:\n",
    "    # https://elbruno.com/2019/05/29/vscode-lets-do-some-facerecognition-with-20-lines-in-python-3-n/    \n",
    "    rgb_frame = frame[:, :, ::-1].copy()\n",
    "    frame_copy = frame.copy()\n",
    "    bw_frame = np.mean(rgb_frame, axis=2)\n",
    "\n",
    "    face_landmarks_list = face_recognition.face_landmarks(rgb_frame)\n",
    "    \n",
    "    # Extract region around eyes, before green lines added. Uses face_recognition\n",
    "    border_height = 10\n",
    "    border_width = 15\n",
    "    \n",
    "    # Creat linear ingredients to bundle with the eye data\n",
    "    grad_x = np.zeros(frame_copy.shape[:2], dtype=np.float)\n",
    "    grad_y = np.zeros(frame_copy.shape[:2], dtype=np.float)\n",
    "    \n",
    "    for i in range(border_height * 2):\n",
    "        grad_x[i, :] = i / (border_height * 2)\n",
    "        \n",
    "    for j in range(border_width * 2):\n",
    "        grad_y[:, j] = j / (border_width * 2)\n",
    "    \n",
    "    try:\n",
    "        left_eye = np.mean(np.array(face_landmarks_list[0][\"left_eye\"]), axis=0, dtype=int)\n",
    "        left_eye_region = bw_frame[left_eye[1] - border_height: left_eye[1] + border_height,\n",
    "                                   left_eye[0] - border_width: left_eye[0] + border_width]\n",
    "        left_eye_x_grad = grad_x[left_eye[1] - border_height: left_eye[1] + border_height,\n",
    "                                 left_eye[0] - border_width: left_eye[0] + border_width]\n",
    "        left_eye_y_grad = grad_y[left_eye[1] - border_height: left_eye[1] + border_height,\n",
    "                                 left_eye[0] - border_width: left_eye[0] + border_width]\n",
    "        \n",
    "        left_eye_flattened = left_eye_region.reshape(1,-1)[0]\n",
    "    \n",
    "        right_eye = np.mean(np.array(face_landmarks_list[0][\"right_eye\"]), axis=0, dtype=int)\n",
    "        right_eye_region = bw_frame[right_eye[1] - border_height: right_eye[1] + border_height,\n",
    "                                    right_eye[0] - border_width: right_eye[0] + border_width]\n",
    "        right_eye_x_grad = grad_x[right_eye[1] - border_height: right_eye[1] + border_height,\n",
    "                                  right_eye[0] - border_width: right_eye[0] + border_width]\n",
    "        right_eye_y_grad = grad_y[right_eye[1] - border_height: right_eye[1] + border_height,\n",
    "                                  right_eye[0] - border_width: right_eye[0] + border_width]\n",
    "        \n",
    "        right_eye_flattened = right_eye_region.reshape(1,-1)[0]\n",
    "            \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        left_eye_region = scaler.fit_transform(left_eye_region)\n",
    "        right_eye_region = scaler.fit_transform(right_eye_region)\n",
    "        \n",
    "        eyes_and_gradients = np.stack((left_eye_region, left_eye_x_grad, left_eye_y_grad,\n",
    "                                       right_eye_region, right_eye_x_grad, right_eye_y_grad), axis=2)\n",
    "    except IndexError:\n",
    "        print(\"Could not extract eye regions, probably because face not detected\")\n",
    "        return [], [], [], []\n",
    "        \n",
    "    for face_landmarks in face_landmarks_list:\n",
    "\n",
    "        for facial_feature in face_landmarks.keys():\n",
    "            pts = np.array([face_landmarks[facial_feature]], np.int32) \n",
    "            pts = pts.reshape((-1,1,2))\n",
    "            cv2.polylines(frame, [pts], False, (0,255,0))\n",
    "\n",
    "    if display:\n",
    "        cv2.imshow('Video', frame)\n",
    "        \n",
    "    # print(face_landmarks_list)\n",
    "    \n",
    "    # I suspect this code will break if multiple faces\n",
    "    landmark_array = np.array(np.zeros((0, 2)))\n",
    "    if face_landmarks_list != []:\n",
    "        for landmark in face_landmarks_list[0].values():\n",
    "            landmark_array = np.concatenate((landmark_array, np.array(landmark)))\n",
    "    else:\n",
    "        print(\"No face detected\") \n",
    "    \n",
    "    # Concatenate the extracted facial features, with the region around the eyes \n",
    "    everything_array = np.concatenate(\n",
    "        (landmark_array[0], left_eye_flattened, right_eye_flattened))\n",
    "    landmark_array = landmark_array[0]\n",
    "    \n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "    \n",
    "    everything_array = everything_array.reshape(1, -1)\n",
    "    landmark_array = landmark_array.reshape(1, -1)\n",
    "    \n",
    "    # print(landmark_array[0].shape)\n",
    "    \n",
    "    return rgb_frame, everything_array, landmark_array, eyes_and_gradients\n",
    "\n",
    "def predict_gaze(video_capture, webcam_resolution,  \n",
    "                 tk_width, tk_height, model, model_type, canvas):\n",
    "    \n",
    "    ret, frame = video_capture.read()\n",
    "    (rgb_frame, everything_array, \n",
    "     landmark_array, eyes_and_gradients) = extract_facial_features(frame)\n",
    "    \n",
    "    try:\n",
    "        if model_type == \"neural net\":\n",
    "            X = np.expand_dims(eyes_and_gradients, 0)\n",
    "            predicted_gaze = model.predict(X)[0]\n",
    "        else:\n",
    "            predicted_gaze = model.predict(everything_array)[0]\n",
    "    \n",
    "        print(\"Predicted gaze is: \", predicted_gaze)\n",
    "    except ValueError:\n",
    "        print(\"Could not predict, probably no face in image\")\n",
    "        predicted_gaze = np.array([0., 0.])\n",
    "    \n",
    "    # Scale the prediction to webcam resolution\n",
    "    predicted_pixel = [predicted_gaze[0] * tk_width, predicted_gaze[1] * tk_height]\n",
    "    # print(predicted_pixel, predicted_gaze, webcam_resolution)\n",
    "    \n",
    "    # Display the prediction as a grey circle\n",
    "    small_dot(canvas, predicted_pixel[0], predicted_pixel[1], radius=5, fill=\"grey\")\n",
    "    \n",
    "    return rgb_frame, everything_array, eyes_and_gradients, predicted_gaze\n",
    "\n",
    "def capture(counter, canvas, model, model_type, training_X, training_y, tk_width, tk_height, \n",
    "            video_capture, rgb_frame, webcam_resolution, \n",
    "            landmark_array, eyes_and_gradients, current_target, predicted_gaze, move_smoothly=False, randomise_dot=True):\n",
    "    \"\"\"Will capture an image, coordinate pair when the user is looking at the dot\"\"\"\n",
    "    \n",
    "    path = \"data/MZeina_5/\"\n",
    "    train_every = 1\n",
    "        \n",
    "    # print(\"About to learn...\")\n",
    "    if len(landmark_array) != 0:\n",
    "        current_target = np.array(current_target) / np.array([tk_width, tk_height])\n",
    "        \n",
    "        if model_type == \"neural net\":\n",
    "            # Neural network can train on each sample at a time, unlike random forest\n",
    "            training_X = np.expand_dims(eyes_and_gradients, 0)\n",
    "            training_y = np.expand_dims(current_target, 0)\n",
    "            # training_X.append(eyes_and_gradients)\n",
    "        else:\n",
    "            training_X.append(landmark_array[0])\n",
    "            training_y.append(current_target)\n",
    "        \n",
    "        plt.imsave(path + str(current_target) + \".jpg\", rgb_frame)\n",
    "        \n",
    "        if counter % train_every == 0:\n",
    "            model.fit(training_X, training_y)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not detected, will not train on this sample\")\n",
    "    \n",
    "    #canvas.delete(\"all\")\n",
    "    if move_smoothly:\n",
    "        speed = 20\n",
    "        scaled_counter = (counter * speed) % (tk_width * tk_height)\n",
    "        target_x = (scaled_counter // tk_height * speed) % tk_width\n",
    "        if (scaled_counter // tk_height)%2 == 0:\n",
    "            target_y = scaled_counter % tk_height\n",
    "        else:\n",
    "            # reverse the direction for alternative lines, so it doesn't skip up to the top\n",
    "            target_y = tk_height - scaled_counter % tk_height\n",
    "        print(\"counter, scaled_counter, are :\", counter, scaled_counter)\n",
    "        print(\"about to move small circle to\", target_x, target_y)\n",
    "        small_dot(canvas, target_x, target_y)\n",
    "        current_target = [target_x, target_y]\n",
    "    elif randomise_dot:\n",
    "        current_target = random_dot(canvas, tk_width, tk_height)\n",
    "    # print(random_width, random_height)\n",
    "    \n",
    "    return model, current_target\n",
    "\n",
    "def train_retrospectively(path_to_images, model):\n",
    "    \n",
    "    # Build data frame of past images, and the extract features\n",
    "    # For any non-small neural network, I should replace this technique with a generator\n",
    "    \n",
    "    training_X = []\n",
    "    training_y = []\n",
    "    counter = 0\n",
    "    path_to_images = \"captures_one/\"\n",
    "    \n",
    "    # Currently only looks in a single directory\n",
    "    files = os.listdir(path_to_images)\n",
    "    \n",
    "    for file in files:\n",
    "        print(\"About to process image number \", counter)\n",
    "        image = cv2.imread(path_to_images + file)\n",
    "        rgb_frame, everything_array, landmark_array, eyes_and_gradients = extract_facial_features(image)\n",
    "        coordinates = [float(coordinate) for coordinate in file[1: -5].split(\" \") if len(coordinate) != 0]\n",
    "        \n",
    "        training_X.append(eyes_and_gradients)\n",
    "        training_y.append(coordinates)\n",
    "        \n",
    "        counter += 1\n",
    "                       \n",
    "    return training_X, training_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that leverage the above to do something useful\n",
    "def train_and_preview(pretrained_model=None):\n",
    "    ########## Universal Initialisation ##########\n",
    "    counter = 0\n",
    "    captures_per_point = 5\n",
    "    \n",
    "    ########## Initialise Video Stream ##########\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Extract webcam resolution\n",
    "    ret, frame = video_capture.read()\n",
    "    webcam_resolution = frame.shape[:2]\n",
    "    # print(webcam_resolution) \n",
    "    \n",
    "    ########## Initialise ML Model ##########\n",
    "    \n",
    "    # Dummy sample, to help initialising models\n",
    "    (rgb_frame, dummy_features, \n",
    "     landmark_array, eyes_and_gradients) = extract_facial_features(frame)\n",
    "    \n",
    "    model_type = \"neural net\"\n",
    "    \n",
    "    if pretrained_model:\n",
    "        model = pretrained_model\n",
    "    elif model_type == \"random forest\":\n",
    "        # Random forest \n",
    "        RF = RandomForestRegressor(n_estimators=500, n_jobs=-1, warm_start=False)\n",
    "        model = MultiOutputRegressor(RF)\n",
    "        model.fit(np.zeros_like(dummy_features), np.array([0.5, 0.5]).reshape(1, -1))\n",
    "    elif model_type == \"neural net\":\n",
    "        model = neural_model(eyes_and_gradients)\n",
    "        model.summary()\n",
    "        \n",
    "    # To do:Train on existing pictures\n",
    "    \n",
    "    # Initialise\n",
    "    training_X = []\n",
    "    training_y = []\n",
    "    \n",
    "    ########## Initialise Tkinter ##########\n",
    "    window = Tk()\n",
    "    window.attributes(\"-fullscreen\", True)\n",
    "    \n",
    "    window.update_idletasks() \n",
    "    tk_width = window.winfo_width() \n",
    "    tk_height = window.winfo_height()\n",
    "\n",
    "    canvas = Canvas(window, width = tk_width, height = tk_height)\n",
    "    canvas.pack()\n",
    "    \n",
    "    window.bind(\"<F11>\", lambda event: window.attributes(\"-fullscreen\",\n",
    "                                        not window.attributes(\"-fullscreen\")))\n",
    "    window.bind(\"<Escape>\", lambda event: window.attributes(\"-fullscreen\", False))\n",
    "    # window.bind(\"c\", lambda event: capture(canvas, RFMO, tk_width, tk_height, video_capture, webcam_resolution, landmark_array, current_target, predicted_gaze))\n",
    "    \n",
    "    # Variables to store red dot target\n",
    "    current_target = random_dot(canvas, tk_width, tk_height)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        rgb_frame, landmark_array, eyes_and_gradients, predicted_gaze = predict_gaze(\n",
    "            video_capture, webcam_resolution, tk_width, tk_height, model, model_type, canvas)\n",
    "        \n",
    "        if counter % 4 == 0 and counter != 0:\n",
    "            canvas.delete(\"all\")\n",
    "            \n",
    "            RFMO, current_target = capture(\n",
    "                counter, canvas, model, model_type, training_X, training_y, tk_width, tk_height, video_capture, \n",
    "                rgb_frame, webcam_resolution, landmark_array, eyes_and_gradients, \n",
    "                current_target, predicted_gaze, randomise_dot=True)\n",
    "                \n",
    "        counter += 1\n",
    "        \n",
    "        # Update GUI\n",
    "        window.update_idletasks()\n",
    "        window.update()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_preview(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "RF = RandomForestRegressor(n_estimators=100, n_jobs=-1, warm_start=True)\n",
    "RFMO = MultiOutputRegressor(RF)\n",
    "RFMO.fit(np.zeros_like(extract_facial_features(video_capture)), np.array([0, 0]).reshape(1, -1))\n",
    "# RFMO.predict(np.array([1,1,1]).reshape(1, -1))\n",
    "# RFMO.fit(np.array([1,1,1]).reshape(1, -1), np.array([1, 0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScreenshotGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, path_to_images, batch_size=4):\n",
    "        \n",
    "        self.path_to_images = path_to_images\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        self.files = []# os.listdir(path_to_images)\n",
    "        self.filenames = []\n",
    "        \n",
    "        for root, dirs, files in os.walk(path_to_images):\n",
    "            for name in files:\n",
    "                self.files.append(os.path.join(root, name))\n",
    "                self.filenames.append(name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.files) // self.batch_size\n",
    "    \n",
    "    def __load__(self, index):\n",
    "        \"\"\"Returns and processes a single sample, in conjunction with __getitem__\"\"\"\n",
    "        \n",
    "        # Ensures that if an image is picked without a succesfully detected face, \n",
    "        #  it looks for another random one to replace it\n",
    "        got_good_image = False\n",
    "        \n",
    "        while not got_good_image:\n",
    "        \n",
    "            file = self.files[index]\n",
    "            filename = self.filenames[index]\n",
    "                        \n",
    "            image = cv2.imread(file)\n",
    "            \n",
    "            rgb_frame, everything_array, landmark_array, eyes_and_gradients = extract_facial_features(image)\n",
    "            coordinates = [float(coordinate) for coordinate in filename[1: -5].split(\" \") if len(coordinate) != 0]\n",
    "            \n",
    "            X = eyes_and_gradients\n",
    "            y = coordinates\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                print(\"This image did not have a recognisable face, will pull a random one in its place\")\n",
    "                index = random.randint(0, self.__len__())\n",
    "            else:\n",
    "                got_good_image = True\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __getitem__(self, batch):\n",
    "        \n",
    "        batch_X = [self.__load__(index)[0] for index in \n",
    "                   range((batch * self.batch_size), (batch + 1) * self.batch_size)]\n",
    "        batch_y = [self.__load__(index)[1] for index in \n",
    "                   range((batch * self.batch_size), (batch + 1) * self.batch_size)]\n",
    "        \n",
    "        batch_X = np.array(batch_X)\n",
    "        batch_y = np.array(batch_y)\n",
    "        \n",
    "        return batch_X, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "_, frame = video_capture.read()\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rgb_frame, dummy_features, \n",
    "     landmark_array, eyes_and_gradients) = extract_facial_features(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to initialise a neural network with input shape:  (20, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "model = neural_model(eyes_and_gradients)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "  98/1041 [=>............................] - ETA: 1:22:27 - loss: 0.0606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-bf2c55c46f50>\", line 2, in <module>\n",
      "    model.fit_generator(screenshot_generator, epochs=4, max_queue_size=100)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1718, in fit_generator\n",
      "    return training_generator.fit_generator(\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\engine\\training_generator.py\", line 185, in fit_generator\n",
      "    generator_output = next(output_generator)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 610, in get\n",
      "    inputs = future.get(timeout=30)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\multiprocessing\\pool.py\", line 762, in get\n",
      "    self.wait(timeout)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\multiprocessing\\pool.py\", line 759, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\threading.py\", line 558, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\moham\\Anaconda3\\envs\\face_recognition_env\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "screenshot_generator = ScreenshotGenerator(\"data/\", 8)\n",
    "model.fit_generator(screenshot_generator, epochs=4, max_queue_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
