{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moby Eye Tracking\n",
    "\n",
    "Notebook for developing fast, accurate eye tracking straight from your webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import face_recognition\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "from tkinter import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras \n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, Dense, MaxPool2D, Flatten \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience functions\n",
    "def small_dot(tkinter_canvas, centre_x, centre_y, radius=5, fill=\"red\"):\n",
    "    \"\"\"Given the centre point of a dot, this convenience function will draw a small dot with given radius\"\"\"\n",
    "    \n",
    "    tkinter_canvas.create_oval(centre_x - radius, centre_y - radius,\n",
    "                               centre_x + radius, centre_y + radius, fill=fill)\n",
    "    \n",
    "    return\n",
    "\n",
    "def random_dot(tkinter_canvas, tk_width, tk_height):\n",
    "    \n",
    "    border = 5 # Should be same, or higher than radius of dots\n",
    "    \n",
    "    random_width = random.randint(border, tk_width - border)\n",
    "    random_height = random.randint(border, tk_height - border)\n",
    "    \n",
    "    small_dot(tkinter_canvas, random_width, random_height)\n",
    "    \n",
    "    return random_width, random_height\n",
    "\n",
    "def neural_model(dummy_sample):\n",
    "    \n",
    "    print(\"About to initialise a neural network with input shape: \", dummy_sample.shape)\n",
    "    \n",
    "    visible = Input(shape=(dummy_sample.shape))\n",
    "    \n",
    "    c11 = Conv2D(4, 3)(visible)\n",
    "    c12 = Conv2D(4, 3)(c11)\n",
    "    p1 = Conv2D(8, 1, strides=2)(c12)\n",
    "    c21 = Conv2D(8, 3)(p1)\n",
    "    c22 = Conv2D(8, 3)(c21)\n",
    "    p2 = Conv2D(16, 1, strides=2)(c22)\n",
    "    #c31 = Conv2D(8, 3)(p2)\n",
    "    #c32 = Conv2D(8, 3)(c31)\n",
    "    #p3 = Conv2D(16, 1, strides=2)(c32)\n",
    "    \n",
    "    f1 = Flatten()(p2)\n",
    "    d1 = Dense(200, activation=\"relu\")(f1)\n",
    "    d2 = Dense(200, activation=\"relu\")(d1)\n",
    "    output = Dense(2)(d2)\n",
    "    \n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    \n",
    "    model.compile(loss=keras.losses.MeanSquaredError(), optimizer=\"adam\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_facial_features(frame, display=False):\n",
    "    \n",
    "    # Basic code for facial landmark extraction from webcam from:\n",
    "    # https://elbruno.com/2019/05/29/vscode-lets-do-some-facerecognition-with-20-lines-in-python-3-n/    \n",
    "    rgb_frame = frame[:, :, ::-1].copy()\n",
    "    frame_copy = frame.copy()\n",
    "    bw_frame = np.mean(rgb_frame, axis=2)\n",
    "\n",
    "    face_landmarks_list = face_recognition.face_landmarks(rgb_frame)\n",
    "    \n",
    "    # Extract region around eyes, before green lines added. Uses face_recognition\n",
    "    border_height = 10\n",
    "    border_width = 15\n",
    "    \n",
    "    # Creat linear ingredients to bundle with the eye data\n",
    "    grad_x = np.zeros(frame_copy.shape[:2], dtype=np.float)\n",
    "    grad_y = np.zeros(frame_copy.shape[:2], dtype=np.float)\n",
    "    \n",
    "    for i in range(border_height * 2):\n",
    "        grad_x[i, :] = i / (border_height * 2)\n",
    "        \n",
    "    for j in range(border_width * 2):\n",
    "        grad_y[:, j] = j / (border_width * 2)\n",
    "    \n",
    "    try:\n",
    "        left_eye = np.mean(np.array(face_landmarks_list[0][\"left_eye\"]), axis=0, dtype=int)\n",
    "        left_eye_region = bw_frame[left_eye[1] - border_height: left_eye[1] + border_height,\n",
    "                                   left_eye[0] - border_width: left_eye[0] + border_width]\n",
    "        left_eye_x_grad = grad_x[left_eye[1] - border_height: left_eye[1] + border_height,\n",
    "                                 left_eye[0] - border_width: left_eye[0] + border_width]\n",
    "        left_eye_y_grad = grad_y[left_eye[1] - border_height: left_eye[1] + border_height,\n",
    "                                 left_eye[0] - border_width: left_eye[0] + border_width]\n",
    "        \n",
    "        left_eye_flattened = left_eye_region.reshape(1,-1)[0]\n",
    "    \n",
    "        right_eye = np.mean(np.array(face_landmarks_list[0][\"right_eye\"]), axis=0, dtype=int)\n",
    "        right_eye_region = bw_frame[right_eye[1] - border_height: right_eye[1] + border_height,\n",
    "                                    right_eye[0] - border_width: right_eye[0] + border_width]\n",
    "        right_eye_x_grad = grad_x[right_eye[1] - border_height: right_eye[1] + border_height,\n",
    "                                  right_eye[0] - border_width: right_eye[0] + border_width]\n",
    "        right_eye_y_grad = grad_y[right_eye[1] - border_height: right_eye[1] + border_height,\n",
    "                                  right_eye[0] - border_width: right_eye[0] + border_width]\n",
    "        \n",
    "        right_eye_flattened = right_eye_region.reshape(1,-1)[0]\n",
    "            \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        left_eye_region = scaler.fit_transform(left_eye_region)\n",
    "        right_eye_region = scaler.fit_transform(right_eye_region)\n",
    "        \n",
    "        eyes_and_gradients = np.stack((left_eye_region, left_eye_x_grad, left_eye_y_grad,\n",
    "                                       right_eye_region, right_eye_x_grad, right_eye_y_grad), axis=2)\n",
    "    except IndexError:\n",
    "        print(\"Could not extract eye regions, probably because face not detected\")\n",
    "        return [], [], [], []\n",
    "        \n",
    "    for face_landmarks in face_landmarks_list:\n",
    "\n",
    "        for facial_feature in face_landmarks.keys():\n",
    "            pts = np.array([face_landmarks[facial_feature]], np.int32) \n",
    "            pts = pts.reshape((-1,1,2))\n",
    "            cv2.polylines(frame, [pts], False, (0,255,0))\n",
    "\n",
    "    if display:\n",
    "        cv2.imshow('Video', frame)\n",
    "        \n",
    "    # print(face_landmarks_list)\n",
    "    \n",
    "    # I suspect this code will break if multiple faces\n",
    "    landmark_array = np.array(np.zeros((0, 2)))\n",
    "    if face_landmarks_list != []:\n",
    "        for landmark in face_landmarks_list[0].values():\n",
    "            landmark_array = np.concatenate((landmark_array, np.array(landmark)))\n",
    "    else:\n",
    "        print(\"No face detected\") \n",
    "    \n",
    "    # Concatenate the extracted facial features, with the region around the eyes \n",
    "    everything_array = np.concatenate(\n",
    "        (landmark_array[0], left_eye_flattened, right_eye_flattened))\n",
    "    landmark_array = landmark_array[0]\n",
    "    \n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "    \n",
    "    everything_array = everything_array.reshape(1, -1)\n",
    "    landmark_array = landmark_array.reshape(1, -1)\n",
    "    \n",
    "    # print(landmark_array[0].shape)\n",
    "    \n",
    "    return rgb_frame, everything_array, landmark_array, eyes_and_gradients\n",
    "\n",
    "def predict_gaze(video_capture, webcam_resolution,  \n",
    "                 tk_width, tk_height, model, model_type, canvas):\n",
    "    \n",
    "    ret, frame = video_capture.read()\n",
    "    (rgb_frame, everything_array, \n",
    "     landmark_array, eyes_and_gradients) = extract_facial_features(frame)\n",
    "    \n",
    "    try:\n",
    "        if model_type == \"neural net\":\n",
    "            X = np.expand_dims(eyes_and_gradients, 0)\n",
    "            predicted_gaze = model.predict(X)[0]\n",
    "        else:\n",
    "            predicted_gaze = model.predict(everything_array)[0]\n",
    "    \n",
    "        print(\"Predicted gaze is: \", predicted_gaze)\n",
    "    except ValueError:\n",
    "        print(\"Could not predict, probably no face in image\")\n",
    "        predicted_gaze = np.array([0., 0.])\n",
    "    \n",
    "    # Scale the prediction to webcam resolution\n",
    "    predicted_pixel = [predicted_gaze[0] * tk_width, predicted_gaze[1] * tk_height]\n",
    "    # print(predicted_pixel, predicted_gaze, webcam_resolution)\n",
    "    \n",
    "    # Display the prediction as a grey circle\n",
    "    small_dot(canvas, predicted_pixel[0], predicted_pixel[1], radius=5, fill=\"grey\")\n",
    "    \n",
    "    return rgb_frame, everything_array, eyes_and_gradients, predicted_gaze\n",
    "\n",
    "def capture(counter, canvas, model, model_type, training_X, training_y, tk_width, tk_height, \n",
    "            video_capture, rgb_frame, webcam_resolution, \n",
    "            landmark_array, eyes_and_gradients, current_target, predicted_gaze, move_smoothly=False, randomise_dot=True):\n",
    "    \"\"\"Will capture an image, coordinate pair when the user is looking at the dot\"\"\"\n",
    "    \n",
    "    path = \"captures_one/\"\n",
    "    train_every = 1\n",
    "        \n",
    "    # print(\"About to learn...\")\n",
    "    if len(landmark_array) != 0:\n",
    "        current_target = np.array(current_target) / np.array([tk_width, tk_height])\n",
    "        \n",
    "        if model_type == \"neural net\":\n",
    "            # Neural network can train on each sample at a time, unlike random forest\n",
    "            training_X = np.expand_dims(eyes_and_gradients, 0)\n",
    "            training_y = np.expand_dims(current_target, 0)\n",
    "            # training_X.append(eyes_and_gradients)\n",
    "        else:\n",
    "            training_X.append(landmark_array[0])\n",
    "            training_y.append(current_target)\n",
    "        \n",
    "        plt.imsave(path + str(current_target) + \".jpg\", rgb_frame)\n",
    "        \n",
    "        if counter % train_every == 0:\n",
    "            model.fit(training_X, training_y)\n",
    "        \n",
    "    else:\n",
    "        print(\"Face not detected, will not train on this sample\")\n",
    "    \n",
    "    #canvas.delete(\"all\")\n",
    "    if move_smoothly:\n",
    "        speed = 20\n",
    "        scaled_counter = (counter * speed) % (tk_width * tk_height)\n",
    "        target_x = (scaled_counter // tk_height * speed) % tk_width\n",
    "        if (scaled_counter // tk_height)%2 == 0:\n",
    "            target_y = scaled_counter % tk_height\n",
    "        else:\n",
    "            # reverse the direction for alternative lines, so it doesn't skip up to the top\n",
    "            target_y = tk_height - scaled_counter % tk_height\n",
    "        print(\"counter, scaled_counter, are :\", counter, scaled_counter)\n",
    "        print(\"about to move small circle to\", target_x, target_y)\n",
    "        small_dot(canvas, target_x, target_y)\n",
    "        current_target = [target_x, target_y]\n",
    "    elif randomise_dot:\n",
    "        current_target = random_dot(canvas, tk_width, tk_height)\n",
    "    # print(random_width, random_height)\n",
    "    \n",
    "    return model, current_target\n",
    "\n",
    "def train_retrospectively(path_to_images, model):\n",
    "    \n",
    "    # Build data frame of past images, and the extract features\n",
    "    # For any non-small neural network, I should replace this technique with a generator\n",
    "    \n",
    "    training_X = []\n",
    "    training_y = []\n",
    "    counter = 0\n",
    "    path_to_images = \"captures_one/\"\n",
    "    \n",
    "    # Currently only looks in a single directory\n",
    "    files = os.listdir(path_to_images)\n",
    "    \n",
    "    for file in files:\n",
    "        print(\"About to process image number \", counter)\n",
    "        image = cv2.imread(path_to_images + file)\n",
    "        rgb_frame, everything_array, landmark_array, eyes_and_gradients = extract_facial_features(image)\n",
    "        coordinates = [float(coordinate) for coordinate in file[1: -5].split(\" \") if len(coordinate) != 0]\n",
    "        \n",
    "        training_X.append(eyes_and_gradients)\n",
    "        training_y.append(coordinates)\n",
    "        \n",
    "        counter += 1\n",
    "                       \n",
    "    return training_X, training_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that leverage the above to do something useful\n",
    "def train_and_preview(pretrained_model=None):\n",
    "    ########## Universal Initialisation ##########\n",
    "    counter = 0\n",
    "    captures_per_point = 5\n",
    "    \n",
    "    ########## Initialise Video Stream ##########\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Extract webcam resolution\n",
    "    ret, frame = video_capture.read()\n",
    "    webcam_resolution = frame.shape[:2]\n",
    "    # print(webcam_resolution) \n",
    "    \n",
    "    ########## Initialise ML Model ##########\n",
    "    \n",
    "    # Dummy sample, to help initialising models\n",
    "    (rgb_frame, dummy_features, \n",
    "     landmark_array, eyes_and_gradients) = extract_facial_features(frame)\n",
    "    \n",
    "    model_type = \"neural net\"\n",
    "    \n",
    "    if pretrained_model:\n",
    "        model = pretrained_model\n",
    "    elif model_type == \"random forest\":\n",
    "        # Random forest \n",
    "        RF = RandomForestRegressor(n_estimators=500, n_jobs=-1, warm_start=False)\n",
    "        model = MultiOutputRegressor(RF)\n",
    "        model.fit(np.zeros_like(dummy_features), np.array([0.5, 0.5]).reshape(1, -1))\n",
    "    elif model_type == \"neural net\":\n",
    "        model = neural_model(eyes_and_gradients)\n",
    "        model.summary()\n",
    "        \n",
    "    # To do:Train on existing pictures\n",
    "    \n",
    "    # Initialise\n",
    "    training_X = []\n",
    "    training_y = []\n",
    "    \n",
    "    ########## Initialise Tkinter ##########\n",
    "    window = Tk()\n",
    "    window.attributes(\"-fullscreen\", True)\n",
    "    \n",
    "    window.update_idletasks() \n",
    "    tk_width = window.winfo_width() \n",
    "    tk_height = window.winfo_height()\n",
    "\n",
    "    canvas = Canvas(window, width = tk_width, height = tk_height)\n",
    "    canvas.pack()\n",
    "    \n",
    "    window.bind(\"<F11>\", lambda event: window.attributes(\"-fullscreen\",\n",
    "                                        not window.attributes(\"-fullscreen\")))\n",
    "    window.bind(\"<Escape>\", lambda event: window.attributes(\"-fullscreen\", False))\n",
    "    # window.bind(\"c\", lambda event: capture(canvas, RFMO, tk_width, tk_height, video_capture, webcam_resolution, landmark_array, current_target, predicted_gaze))\n",
    "    \n",
    "    # Variables to store red dot target\n",
    "    current_target = random_dot(canvas, tk_width, tk_height)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        rgb_frame, landmark_array, eyes_and_gradients, predicted_gaze = predict_gaze(\n",
    "            video_capture, webcam_resolution, tk_width, tk_height, model, model_type, canvas)\n",
    "        \n",
    "        if counter % 4 == 0 and counter != 0:\n",
    "            canvas.delete(\"all\")\n",
    "            \n",
    "            RFMO, current_target = capture(\n",
    "                counter, canvas, model, model_type, training_X, training_y, tk_width, tk_height, video_capture, \n",
    "                rgb_frame, webcam_resolution, landmark_array, eyes_and_gradients, \n",
    "                current_target, predicted_gaze, randomise_dot=True)\n",
    "                \n",
    "        counter += 1\n",
    "        \n",
    "        # Update GUI\n",
    "        window.update_idletasks()\n",
    "        window.update()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted gaze is:  [0.47157687 0.19433278]\n",
      "Predicted gaze is:  [0.1234096 0.6986617]\n",
      "Predicted gaze is:  [0.17976153 0.6709987 ]\n",
      "Predicted gaze is:  [0.3296043  0.24090895]\n",
      "Predicted gaze is:  [0.8566897  0.89419407]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'captures_one/[0.91171875 0.78472222].jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-12301734e0df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_and_preview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-300187a8617a>\u001b[0m in \u001b[0;36mtrain_and_preview\u001b[1;34m(pretrained_model)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             RFMO, current_target = capture(\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtk_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtk_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mrgb_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwebcam_resolution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmark_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meyes_and_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-53d7be2de33a>\u001b[0m in \u001b[0;36mcapture\u001b[1;34m(counter, canvas, model, model_type, training_X, training_y, tk_width, tk_height, video_capture, rgb_frame, webcam_resolution, landmark_array, eyes_and_gradients, current_target, predicted_gaze, move_smoothly, randomise_dot)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mtraining_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_target\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtrain_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimsave\u001b[1;34m(fname, arr, **kwargs)\u001b[0m\n\u001b[0;32m   2064\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1586\u001b[0m             \u001b[0mpil_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m             \u001b[0mpil_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dpi\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m             \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2129\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2131\u001b[1;33m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'captures_one/[0.91171875 0.78472222].jpg'"
     ]
    }
   ],
   "source": [
    "train_and_preview(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "RF = RandomForestRegressor(n_estimators=100, n_jobs=-1, warm_start=True)\n",
    "RFMO = MultiOutputRegressor(RF)\n",
    "RFMO.fit(np.zeros_like(extract_facial_features(video_capture)), np.array([0, 0]).reshape(1, -1))\n",
    "# RFMO.predict(np.array([1,1,1]).reshape(1, -1))\n",
    "# RFMO.fit(np.array([1,1,1]).reshape(1, -1), np.array([1, 0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScreenshotGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, path_to_images, batch_size=4):\n",
    "        \n",
    "        self.path_to_images = path_to_images\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "        self.files = []# os.listdir(path_to_images)\n",
    "        self.filenames = []\n",
    "        \n",
    "        for root, dirs, files in os.walk(path_to_images):\n",
    "            for name in files:\n",
    "                self.files.append(os.path.join(root, name))\n",
    "                self.filenames.append(name)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.files) // self.batch_size\n",
    "    \n",
    "    def __load__(self, index):\n",
    "        \"\"\"Returns and processes a single sample, in conjunction with __getitem__\"\"\"\n",
    "        \n",
    "        # Ensures that if an image is picked without a succesfully detected face, \n",
    "        #  it looks for another random one to replace it\n",
    "        got_good_image = False\n",
    "        \n",
    "        while not got_good_image:\n",
    "        \n",
    "            file = self.files[index]\n",
    "            filename = self.filenames[index]\n",
    "                        \n",
    "            image = cv2.imread(file)\n",
    "            \n",
    "            rgb_frame, everything_array, landmark_array, eyes_and_gradients = extract_facial_features(image)\n",
    "            coordinates = [float(coordinate) for coordinate in filename[1: -5].split(\" \") if len(coordinate) != 0]\n",
    "            \n",
    "            X = eyes_and_gradients\n",
    "            y = coordinates\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                print(\"This image did not have a recognisable face, will pull a random one in its place\")\n",
    "                index = random.randint(0, self.__len__())\n",
    "            else:\n",
    "                got_good_image = True\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __getitem__(self, batch):\n",
    "        \n",
    "        batch_X = [self.__load__(index)[0] for index in \n",
    "                   range((batch * self.batch_size), (batch + 1) * self.batch_size)]\n",
    "        batch_y = [self.__load__(index)[1] for index in \n",
    "                   range((batch * self.batch_size), (batch + 1) * self.batch_size)]\n",
    "        \n",
    "        batch_X = np.array(batch_X)\n",
    "        batch_y = np.array(batch_y)\n",
    "        \n",
    "        return batch_X, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "_, frame = video_capture.read()\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rgb_frame, dummy_features, \n",
    "     landmark_array, eyes_and_gradients) = extract_facial_features(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to initialise a neural network with input shape:  (20, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "model = neural_model(eyes_and_gradients)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " 860/5687 [===>..........................] - ETA: 2:57:40 - loss: 0.0200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-fe9c708953f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscreenshot_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScreenshotGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eye_tracking_data/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreenshot_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1716\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1717\u001b[0m         \"\"\"\n\u001b[1;32m-> 1718\u001b[1;33m         return training_generator.fit_generator(\n\u001b[0m\u001b[0;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1720\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    608\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\face_recognition_env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "screenshot_generator = ScreenshotGenerator(\"eye_tracking_data/\", 4)\n",
    "model.fit_generator(screenshot_generator, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
